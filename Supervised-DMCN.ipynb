{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DZU9nS-J714K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from copy import deepcopy\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from data_prepare import mirror_hsi\n",
    "from data_prepare import choose_train_and_test\n",
    "from data_prepare import choose_all_pixels, all_data\n",
    "from data_prepare import train_and_test_data, train_and_test_label\n",
    "\n",
    "\n",
    "from Utils import AverageMeter, accuracy\n",
    "from Utils import output_metric, plot_confusion_matrix\n",
    "from Utils import list_to_colormap, classification_map, print_args\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"DMCN\")\n",
    "parser.add_argument('--dataset', choices=['HU2013', 'Salinas', 'HongHu', 'KSC'], default='Salinas', help='dataset to use')\n",
    "parser.add_argument('--seed', type=int, default=42, help='number of seed')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='number of batch size')\n",
    "parser.add_argument('--patch_size', type=int, default=15, help='size of patches')\n",
    "parser.add_argument('--epoches', type=int, default=100, help='epoch number')\n",
    "parser.add_argument('--learning_rate', type=float, default=5e-3, help='learning rate')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='gamma')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001, help='weight_decay')\n",
    "parser.add_argument('--train_number', type=int, default=10, help='num_train_per_class')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class numbers of the HSI data is: 16\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "data_path = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "if args.dataset == 'HU2013':\n",
    "    data = sio.loadmat(os.path.join(data_path, 'Houston.mat'))['Houston']\n",
    "    label = sio.loadmat(os.path.join(data_path, 'Houston_gt.mat'))['Houston_gt']\n",
    "elif args.dataset == 'HanChuan':\n",
    "    data = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HanChuan.mat'))['WHU_Hi_HanChuan']\n",
    "    label = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HanChuan_gt.mat'))['WHU_Hi_HanChuan_gt']\n",
    "elif args.dataset == 'KSC':\n",
    "    data = sio.loadmat(os.path.join(data_path, 'KSC.mat'))['KSC']\n",
    "    label = sio.loadmat(os.path.join(data_path, 'KSC_gt.mat'))['KSC_gt']\n",
    "elif args.dataset == 'Salinas':\n",
    "    data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "    label = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "num_classes = np.max(label)\n",
    "print('The class numbers of the HSI data is:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeor = data.shape\n",
    "data = data.reshape(np.prod(data.shape[:2]), np.prod(data.shape[2:]))\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_data = std_scaler.fit_transform(data)\n",
    "data = std_data.reshape(shapeor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPCA(data, numComponents=30):\n",
    "    new_data = np.reshape(data, (-1, data.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    new_data = pca.fit_transform(new_data)\n",
    "    new_data = np.reshape(new_data, (data.shape[0], data.shape[1], numComponents))\n",
    "    return new_data, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height=512, width=217, band=30\n"
     ]
    }
   ],
   "source": [
    "K = 30\n",
    "data, pca = applyPCA(data, numComponents=K)\n",
    "\n",
    "# data size\n",
    "height, width, band = data.shape\n",
    "print(\"height={0}, width={1}, band={2}\".format(height, width, band))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "patch_size : 15\n",
      "mirror_data shape : [526, 231, 30]\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "mirror_data = mirror_hsi(height, width, band, data, patch_size=args.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "x_train shape = (160, 15, 15, 30), type = float64\n",
      "x_test  shape = (53969, 15, 15, 30), type = float64\n",
      "x_valid  shape = (1600, 15, 15, 30), type = float64\n",
      "*******************************************************\n",
      "y_train: shape = (160,), type = int64\n",
      "y_test: shape = (53969,), type = int64\n",
      "y_valid: shape = (1600,), type = int64\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "total_pos_train, total_pos_test, total_pos_valid, number_train, number_test, number_valid = choose_train_and_test(label, args.train_number, args.seed) \n",
    "\n",
    "x_train, x_test, x_valid = train_and_test_data(mirror_data, band, total_pos_train, total_pos_test, total_pos_valid, args.patch_size)\n",
    "y_train, y_test, y_valid = train_and_test_label(number_train, number_test, number_valid, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[268,  24],\n",
       "       [245,  47],\n",
       "       [242,  56],\n",
       "       [263,  38],\n",
       "       [243,  49],\n",
       "       [246,  49],\n",
       "       [257,  21],\n",
       "       [243,  45],\n",
       "       [247,  36],\n",
       "       [245,  51]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pos_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 30, 15, 15])\n",
      "torch.Size([53969, 30, 15, 15])\n",
      "torch.Size([1600, 30, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train = torch.from_numpy(x_train.transpose(0, 3, 1, 2)).type(torch.FloatTensor)  # (90, 1, 7, 7, 176)\n",
    "print(x_train.shape)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)  # (13,)\n",
    "train_label = Data.TensorDataset(x_train, y_train)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.transpose(0, 3, 1, 2)).type(torch.FloatTensor)  # (5198, 1, 7, 7, 176)\n",
    "print(x_test.shape)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)  # (5198,)\n",
    "test_label = Data.TensorDataset(x_test, y_test)\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid.transpose(0, 3, 1, 2)).type(torch.FloatTensor)  # (5211, 1, 7, 7, 176)\n",
    "print(x_valid.shape)\n",
    "y_valid = torch.from_numpy(y_valid).type(torch.LongTensor)\n",
    "valid_label = Data.TensorDataset(x_valid, y_valid)\n",
    "\n",
    "train_loader = Data.DataLoader(train_label, batch_size=32, shuffle=True)\n",
    "test_loader = Data.DataLoader(test_label, batch_size=128, shuffle=True)\n",
    "valid_loader = Data.DataLoader(valid_label, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D_5INAM4u33"
   },
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBmkAG3_43c-"
   },
   "source": [
    "#### Capsule Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qp1hds7dy_9v"
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \n",
    "    def __init__(self, A=64, B=32, K=1, P=4, stride=1):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.pose = nn.Conv2d(in_channels=A, out_channels=B*P*P, kernel_size=K, stride=stride, bias=True)\n",
    "        self.a = nn.Conv2d(in_channels=A, out_channels=B,\n",
    "                            kernel_size=K, stride=stride, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.pose(x)  # (batch, 32x4x4, 13, 13)\n",
    "        a = self.a(x)  # (batch, 32, 13, 13)\n",
    "        a = self.sigmoid(a)\n",
    "        out = torch.cat([p, a], dim=1)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out  # (batch, 13, 13, 544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ey4FeNfp42l7"
   },
   "outputs": [],
   "source": [
    "class ConvCaps(nn.Module):\n",
    "    def __init__(self, B=32, C=16, K=3, P=4, stride=2, iters=3, coor_add=False, w_shared=False):\n",
    "        super(ConvCaps, self).__init__()\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        self.psize = P * P  # 16\n",
    "        self.stride = stride\n",
    "        self.iters = iters  # 3\n",
    "        self.coor_add = coor_add\n",
    "        self.w_shared = w_shared\n",
    "        self.eps = 1e-07\n",
    "        self._lambda = 1e-02\n",
    "        self.ln_2pi = torch.cuda.FloatTensor(1).fill_(math.log(2*math.pi))  # tensor([1.8379])\n",
    "        \n",
    "        self.beta_u = nn.Parameter(torch.zeros(C))\n",
    "        self.beta_a = nn.Parameter(torch.zeros(C))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 4, 4, B, C, P*P))\n",
    "        self.weights = nn.Parameter(torch.randn(1, K*K*B, C, P, P))  # (1, 3x3x32, 16, 4, 4) 类似胶囊网络中的权重 or (1, 1x1x16, 16, 4, 4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def add_patches(self, x, B, K, psize, stride):\n",
    "        b, h, w, c = x.shape  # (batch, 13, 13, 32*4*4+32)\n",
    "        assert h == w\n",
    "        assert c == B*(psize+1)\n",
    "        oh = ow = int((h - K)/stride + 1)  # 6\n",
    "        # [[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8], [8, 9, 10], [10, 11, 12]]\n",
    "        idxs = [[(h_idx + k_idx) for k_idx in range(0, K)] for h_idx in range(0, h - K + 1, stride)]\n",
    "        x = x[:, idxs, :, :]  # (batch, 6, 3, 13, 544)\n",
    "        x = x[:, :, :, idxs, :]  # (batch, 6, 3, 6, 3, 544)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()  # (batch, 6, 6, 3, 3, 544)\n",
    "        return x, oh, ow  # (batch, 6, 6, 3, 3, 544), 6, 6\n",
    "\n",
    "    def transform_view(self, p_in, weights, C, P, w_shared=False):\n",
    "        b, B, psize = p_in.shape  # (batchx6x6, 3x3x32, 16) or (batch, 4x4x16, 16) when stride=1\n",
    "        assert psize == P*P\n",
    "        p_in = p_in.view(b, B, 1, P, P)  # (batchx6x6, 3x3x32, 1, 4, 4) or (batch, 4x4x16, 1, 4, 4) \n",
    "        if w_shared:  # 所谓的权重共享并没有什么真正的意义，只是为了消除空间尺寸，获得最后的类别胶囊\n",
    "            hw = int(B / weights.size(1))  # int(4x4x16 / 1x1x16) = 4x4\n",
    "            weights = weights.repeat(1, hw, 1, 1, 1)  # (1, 4x4x16, 16, 4, 4)\n",
    "        weights = weights.repeat(b, 1, 1, 1, 1)  # (batchx6x6, 3x3x32, 16, 4, 4) or (batch, 4x4x16, 16, 4, 4) \n",
    "        p_in = p_in.repeat(1, 1, C, 1, 1)  # (batchx6x6, 3x3x32, 16, 4, 4) or (batch, 4x4x16, 16, 4, 4)\n",
    "        v = torch.matmul(p_in, weights)\n",
    "        v = v.view(b, B, C, P*P)  # (batchx6x6, 3x3x32, 16, 16) 所有初级胶囊i对于所有胶囊j的投票结果 or (batch, 4x4x16, 16, 16)\n",
    "        return v\n",
    "\n",
    "    def add_coord(self, v, b, h, w, B, C, psize):\n",
    "        # v: (batch, 4x4x16, 16, 16)\n",
    "        assert h == w == 4\n",
    "        v = v.view(b, h, w, B, C, psize)  # (batch, 4, 4, 16, 16, 16)\n",
    "        v = v + self.pos_embedding\n",
    "        v = v.view(b, h*w*B, C, psize)  # (batch, 4x4x16, 16, 16)\n",
    "        return v\n",
    "\n",
    "    def m_step(self, a_in, r, v, eps, b, B, C, psize, iter):\n",
    "        # a_in: (batchx6x6, 3x3x32, 1); r: (batchx6x6, 3x3x32, 16); v: (batchx6x6, 3x3x32, 16, 16)\n",
    "        r = r * a_in  # (batchx6x6, 3x3x32, 16) 这里的16表示每个位置的父胶囊的个数\n",
    "        r = r / (r.sum(dim=2, keepdim=True) + eps)  # (batchx6x6, 3x3x32, 16) 这一步的目的是使r重新表示成概率(和为1)\n",
    "        r_sum = r.sum(dim=1, keepdim=True)  # (batchx6x6, 1, 16) 这里的3x3x32可以看作巻积胶囊层中初级胶囊的个数\n",
    "        coeff = r / (r_sum + eps)  # (batchx6x6, 3x3x32, 16)\n",
    "        coeff = coeff.view(b, B, C, 1)  # (batchx6x6, 3x3x32, 16, 1) \n",
    "\n",
    "        mu = torch.sum(coeff * v, dim=1, keepdim=True)  # (batchx6x6, 1, 16, 16)  均值\n",
    "        # 这里的代码实现先计算了一个coeff，对应M-Step中的第二步，并没有错哦，只是有点儿巧妙\n",
    "        sigma_sq = torch.sum(coeff * (v - mu)**2, dim=1, keepdim=True) + eps  # (batchx6x6, 1, 16, 16)  方差\n",
    "\n",
    "        r_sum = r_sum.view(b, C, 1)  # (batchx6x6, 16, 1)\n",
    "        sigma_sq = sigma_sq.view(b, C, psize)  # (batchx6x6, 16, 16)\n",
    "        #cost_h = (0.5 + 0.5*math.log(2*math.pi) + torch.log(sigma_sq.sqrt())) * r_sum  # (batchx6x6, 16, 16)\n",
    "        cost_h = (self.beta_u.view(C, 1) + torch.log(sigma_sq.sqrt())) * r_sum  # (batchx6x6, 16, 16)\n",
    "\n",
    "        #a_out = self.sigmoid((self._lambda*(1-0.95**(iter+1)))*(self.beta_a - self.beta_u * r_sum.view(b, C) - cost_h.sum(dim=2)))  # (batchx6x6, 16)\n",
    "        a_out = self.sigmoid((self._lambda*(1-0.95**(iter+1)))*(self.beta_a - cost_h.sum(dim=2)))  # cost_h.sum(dim=2)表示对每个父胶囊在特征维度求和 (batchx6x6, 16)\n",
    "        sigma_sq = sigma_sq.view(b, 1, C, psize)  # (batchx6x6, 1, 16, 16)\n",
    "\n",
    "        return a_out, mu, sigma_sq  # (batchx6x6, 16), (batchx6x6, 1, 16, 16), (batchx6x6, 1, 16, 16) 父胶囊激活，父胶囊分布的均值，父胶囊分布的方差\n",
    "\n",
    "    def e_step(self, mu, sigma_sq, a_out, v, eps, b, C):\n",
    "        # v: (batchx6x6, 3x3x32, 16, 16)\n",
    "        ln_p_j_h = -1. * (v - mu) ** 2 / (2 * sigma_sq) - torch.log(sigma_sq.sqrt()) - 0.5 * self.ln_2pi  # 太过于精彩!!!!!看到这里你豁然开朗了吗？？？\n",
    "        # 这一步在计算所有初级胶囊的投票与父胶囊分布的均值之间的相似性，根据这个相似性来重新计算概率分配，你，懂了吗？？？ (batchx6x6, 3x3x32, 16, 16) \n",
    "        ln_ap = ln_p_j_h.sum(dim=3) + torch.log(a_out.view(b, 1, C))  # 那既然对p_j_h求对数了，也要对a求对数  (batchx6x6, 3x3x32, 16)\n",
    "        r = self.softmax(ln_ap)  # 重新计算的概率分配 (batchx6x6, 3x3x32, 16)\n",
    "        return r\n",
    "\n",
    "    def caps_em_routing(self, v, a_in, C, eps):\n",
    "        b, B, c, psize = v.shape  # (batchx6x6, 3x3x32, 16, 16)\n",
    "        assert c == C\n",
    "        assert (b, B, 1) == a_in.shape  # (batchx6x6, 3x3x32, 1)\n",
    "\n",
    "        r = torch.cuda.FloatTensor(b, B, C).fill_(1./C)  # (batchx6x6, 3x3x32, 16) 分配概率rij(和为1)\n",
    "        for iter_ in range(self.iters):\n",
    "            a_out, mu, sigma_sq = self.m_step(a_in, r, v, eps, b, B, C, psize, iter_)  \n",
    "            # (batchx6x6, 16), (batchx6x6, 1, 16, 16), (batchx6x6, 1, 16, 16) 父胶囊激活，父胶囊分布的均值，父胶囊分布的方差\n",
    "            if iter_ < self.iters - 1:\n",
    "                r = self.e_step(mu, sigma_sq, a_out, v, eps, b, C)  # 更新分配概率(batchx6x6, 3x3x32, 16)\n",
    "\n",
    "        return mu, a_out  # (batchx6x6, 1, 16, 16), (batchx6x6, 16) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, h, w, c = x.shape  # (batch, 13, 13, 544) or (batch, 4, 4, (16+1)*16)\n",
    "        if not self.w_shared:\n",
    "            x, oh, ow = self.add_patches(x, self.B, self.K, self.psize, self.stride)  # (batch, 6, 6, 3, 3, 544), 6, 6\n",
    "\n",
    "            p_in = x[:, :, :, :, :, :self.B*self.psize].contiguous()  # (batch, 6, 6, 3, 3, 512)\n",
    "            a_in = x[:, :, :, :, :, self.B*self.psize:].contiguous()  #  (batch, 6, 6, 3, 3, 32)\n",
    "            p_in = p_in.view(b*oh*ow, self.K*self.K*self.B, self.psize)  # (batchx6x6, 3x3x32, 16) 这里的3x3x32可以看作巻积胶囊层中初级胶囊的个数\n",
    "            a_in = a_in.view(b*oh*ow, self.K*self.K*self.B, 1)  # (batchx6x6, 3x3x32, 1)\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P)  # (batchx6x6, 3x3x32, 16, 16) 所有初级胶囊对所有父胶囊的投票\n",
    "\n",
    "            p_out, a_out = self.caps_em_routing(v, a_in, self.C, self.eps)  # 返回父胶囊分布的均值(batchx6x6, 1, 16, 16)和父胶囊的激活(batchx6x6, 16)\n",
    "            p_out = p_out.view(b, oh, ow, self.C*self.psize)  # （batch, 6, 6, 16x16）\n",
    "            a_out = a_out.view(b, oh, ow, self.C)  # (batch, 6, 6, 16)\n",
    "            out = torch.cat([p_out, a_out], dim=3)  # (batch, 6, 6, (16+1)x16)\n",
    "        else:\n",
    "            # x: (batch, 4, 4, (16+1)*16)\n",
    "            assert c == self.B*(self.psize+1)\n",
    "            assert 1 == self.K\n",
    "            assert 1 == self.stride\n",
    "            p_in = x[:, :, :, :self.B*self.psize].contiguous()  # (batch, 4, 4, 16*16)\n",
    "            p_in = p_in.view(b, h*w*self.B, self.psize)  # (batch, 4x4x16, 16)\n",
    "            a_in = x[:, :, :, self.B*self.psize:].contiguous()  # (batch, 4, 4, 16)\n",
    "            a_in = a_in.view(b, h*w*self.B, 1)  # (batch, 4x4x16, 1)\n",
    "\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P, self.w_shared)  # (batch, 4x4x16, 16, 16)\n",
    "\n",
    "            if self.coor_add:\n",
    "                v = self.add_coord(v, b, h, w, self.B, self.C, self.psize)\n",
    "            _, out = self.caps_em_routing(v, a_in, self.C, self.eps)  # 返回父胶囊的激活(batch, 16)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GKbBGB1FzGPw"
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "\n",
    "    def __init__(self, A=64, B=32, C=16, D=16, E=16, K=3, P=4, iters=3, pca_components=30):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=pca_components, out_channels=A, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=A, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.primary_caps = PrimaryCaps(A, B, 1, P, stride=1)\n",
    "        self.conv_caps1 = ConvCaps(B, C, K, P, stride=2, iters=iters)\n",
    "        self.conv_caps2 = ConvCaps(C, D, K, P, stride=1, iters=iters)\n",
    "        self.class_caps = ConvCaps(D, E, 1, P, stride=1, iters=iters, coor_add=True, w_shared=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 30, 15, 15)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.primary_caps(x) # (batch, 13, 13, 544)\n",
    "        x = self.conv_caps1(x)  # (batch, 6, 6, (16+1)x16)\n",
    "        x = self.conv_caps2(x)  # (batch, 4, 4, (16+1)x16)\n",
    "        x = self.class_caps(x)  # (batch, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ypxKKRG5hvO"
   },
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4J7NoS985UHc"
   },
   "outputs": [],
   "source": [
    "class SpreadLoss(_Loss):\n",
    "    def __init__(self, m_min=0.2, m_max=0.5, num_class=9):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.m_min = m_min\n",
    "        self.m_max = m_max\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x, target, r):\n",
    "        b, E = x.shape  # batch, 16\n",
    "        assert E == self.num_class\n",
    "        margin = self.m_min + (self.m_max - self.m_min)*r  # 0.2 + (0.5 - 0.2) * r\n",
    "\n",
    "        at = torch.cuda.FloatTensor(b).fill_(0)  # (batch, )\n",
    "        for i, a in enumerate(target):\n",
    "            lb = int(a.item())  # 真实标签，整数值，并不是one-hot\n",
    "            at[i] = x[i][lb]  # 对应真实标签的类别胶囊激活值\n",
    "        at = at.view(b, 1).repeat(1, E)  # (batch, 16)\n",
    "\n",
    "        zeros = x.new_zeros(x.shape)  # (batch, 16)\n",
    "        loss = torch.max(margin - (at - x), zeros)  # (batch, 16)\n",
    "        loss = loss**2\n",
    "        loss = loss.sum() / b - margin**2 \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05AZSkZ16Jxp"
   },
   "source": [
    "## **Training Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    objs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    train_len = len(train_loader)\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(train_loader):\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()   \n",
    "        optimizer.zero_grad()\n",
    "        batch_pred = model(batch_data)  # (batch, 13)\n",
    "    \n",
    "        r = (1.*(batch_idx+1) + (epoch-1)*train_len) / (epoch*train_len)  # 表示当前训练轮次的训练数据的进度条\n",
    "        loss = criterion(batch_pred, batch_target, r)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n",
    "        n = batch_data.shape[0]\n",
    "        objs.update(loss.data, n)  # 计算所有训练样本的平均损失\n",
    "        top1.update(prec1[0].data, n)  # 计算所有训练样本的平均准确率\n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "# test model\n",
    "def test(model, test_loader):\n",
    "    tar = np.array([])\n",
    "    pre = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_data, batch_target) in enumerate(test_loader):\n",
    "            batch_data = batch_data.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            \n",
    "            batch_pred = model(batch_data)  # (batch, 13)\n",
    "            _, pred = batch_pred.topk(1, axis=1)  # (100, 1)\n",
    "            pp = pred.squeeze()  # (100, )\n",
    "            \n",
    "            tar = np.append(tar, batch_target.data.cpu().numpy())\n",
    "            pre = np.append(pre, pp.data.cpu().numpy())\n",
    "    return tar, pre\n",
    "\n",
    "\n",
    "# train model\n",
    "def valid(model, train_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    objs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    train_len = len(train_loader)\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(train_loader):\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()   \n",
    "        \n",
    "        batch_pred = model(batch_data)  # (batch, 13)\n",
    "        \n",
    "        r = (1.*(batch_idx+1) + (epoch-1)*train_len) / (epoch*train_len)  # 表示当前训练轮次的训练数据的进度条\n",
    "        loss = criterion(batch_pred, batch_target, r)\n",
    "        \n",
    "        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n",
    "        n = batch_data.shape[0]\n",
    "        objs.update(loss.data, n)  # 计算所有训练样本的平均损失\n",
    "        top1.update(prec1[0].data, n)  # 计算所有训练样本的平均准确率\n",
    "    return top1.avg, objs.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hRWF2apsMzc"
   },
   "source": [
    "## **Training Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGwX3eimsZmA"
   },
   "source": [
    "#### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_YYc6YxsXZV",
    "outputId": "c124fbdb-9d50-47ca-a201-dcd1dd5022dd"
   },
   "outputs": [],
   "source": [
    "model = CapsNet(A=64, B=32, C=16, D=16, E=num_classes, iters=2).cuda()\n",
    "criterion = SpreadLoss(num_class=num_classes, m_min=0.2, m_max=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr = args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayyIRBgr7goB",
    "outputId": "f52d0130-1a54-4280-ba2a-cd8ac150c929",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch: 001 - train_loss: 6.3565 - train_acc: 6.2500 - valid_loss: 5.3724 - valid_acc: 6.2500\n",
      "val_acc improved from 0.0000 to 6.2500, saving model to DMC.pt\n",
      "Epoch: 002 - train_loss: 8.7740 - train_acc: 7.5000 - valid_loss: 8.1332 - valid_acc: 12.4375\n",
      "val_acc improved from 6.2500 to 12.4375, saving model to DMC.pt\n",
      "Epoch: 003 - train_loss: 9.7007 - train_acc: 18.1250 - valid_loss: 9.2091 - valid_acc: 26.3750\n",
      "val_acc improved from 12.4375 to 26.3750, saving model to DMC.pt\n",
      "Epoch: 004 - train_loss: 10.0887 - train_acc: 30.0000 - valid_loss: 9.6744 - valid_acc: 31.8750\n",
      "val_acc improved from 26.3750 to 31.8750, saving model to DMC.pt\n",
      "Epoch: 005 - train_loss: 10.2122 - train_acc: 41.8750 - valid_loss: 9.8721 - valid_acc: 44.9375\n",
      "val_acc improved from 31.8750 to 44.9375, saving model to DMC.pt\n",
      "Epoch: 006 - train_loss: 10.1762 - train_acc: 53.1250 - valid_loss: 9.8401 - valid_acc: 55.7500\n",
      "val_acc improved from 44.9375 to 55.7500, saving model to DMC.pt\n",
      "Epoch: 007 - train_loss: 9.9676 - train_acc: 69.3750 - valid_loss: 9.6597 - valid_acc: 70.1250\n",
      "val_acc improved from 55.7500 to 70.1250, saving model to DMC.pt\n",
      "Epoch: 008 - train_loss: 9.6472 - train_acc: 78.1250 - valid_loss: 9.3784 - valid_acc: 78.1250\n",
      "val_acc improved from 70.1250 to 78.1250, saving model to DMC.pt\n",
      "Epoch: 009 - train_loss: 9.2348 - train_acc: 80.6250 - valid_loss: 8.9545 - valid_acc: 80.6250\n",
      "val_acc improved from 78.1250 to 80.6250, saving model to DMC.pt\n",
      "Epoch: 010 - train_loss: 8.7309 - train_acc: 92.5000 - valid_loss: 8.4332 - valid_acc: 84.4375\n",
      "val_acc improved from 80.6250 to 84.4375, saving model to DMC.pt\n",
      "Epoch: 011 - train_loss: 8.1387 - train_acc: 93.7500 - valid_loss: 7.8528 - valid_acc: 92.2500\n",
      "val_acc improved from 84.4375 to 92.2500, saving model to DMC.pt\n",
      "Epoch: 012 - train_loss: 7.4661 - train_acc: 98.1250 - valid_loss: 7.2778 - valid_acc: 90.5625\n",
      "val_acc did not improve from 92.2500\n",
      "Epoch: 013 - train_loss: 6.7951 - train_acc: 96.2500 - valid_loss: 6.6201 - valid_acc: 92.9375\n",
      "val_acc improved from 92.2500 to 92.9375, saving model to DMC.pt\n",
      "Epoch: 014 - train_loss: 6.1751 - train_acc: 96.8750 - valid_loss: 6.0291 - valid_acc: 93.9375\n",
      "val_acc improved from 92.9375 to 93.9375, saving model to DMC.pt\n",
      "Epoch: 015 - train_loss: 5.5932 - train_acc: 96.8750 - valid_loss: 5.4838 - valid_acc: 94.5625\n",
      "val_acc improved from 93.9375 to 94.5625, saving model to DMC.pt\n",
      "Epoch: 016 - train_loss: 5.0551 - train_acc: 97.5000 - valid_loss: 5.0154 - valid_acc: 95.5625\n",
      "val_acc improved from 94.5625 to 95.5625, saving model to DMC.pt\n",
      "Epoch: 017 - train_loss: 4.6140 - train_acc: 100.0000 - valid_loss: 4.5944 - valid_acc: 96.1250\n",
      "val_acc improved from 95.5625 to 96.1250, saving model to DMC.pt\n",
      "Epoch: 018 - train_loss: 4.2421 - train_acc: 98.7500 - valid_loss: 4.2663 - valid_acc: 96.5000\n",
      "val_acc improved from 96.1250 to 96.5000, saving model to DMC.pt\n",
      "Epoch: 019 - train_loss: 3.9235 - train_acc: 98.7500 - valid_loss: 4.0012 - valid_acc: 94.0000\n",
      "val_acc did not improve from 96.5000\n",
      "Epoch: 020 - train_loss: 3.6939 - train_acc: 98.1250 - valid_loss: 3.7631 - valid_acc: 96.8125\n",
      "val_acc improved from 96.5000 to 96.8125, saving model to DMC.pt\n",
      "Epoch: 021 - train_loss: 3.4628 - train_acc: 100.0000 - valid_loss: 3.5852 - valid_acc: 96.8750\n",
      "val_acc improved from 96.8125 to 96.8750, saving model to DMC.pt\n",
      "Epoch: 022 - train_loss: 3.3213 - train_acc: 98.1250 - valid_loss: 3.4366 - valid_acc: 97.1250\n",
      "val_acc improved from 96.8750 to 97.1250, saving model to DMC.pt\n",
      "Epoch: 023 - train_loss: 3.2258 - train_acc: 98.1250 - valid_loss: 3.3484 - valid_acc: 96.8750\n",
      "val_acc did not improve from 97.1250\n",
      "Epoch: 024 - train_loss: 3.1397 - train_acc: 98.7500 - valid_loss: 3.3053 - valid_acc: 94.1875\n",
      "val_acc did not improve from 97.1250\n",
      "Epoch: 025 - train_loss: 3.0122 - train_acc: 100.0000 - valid_loss: 3.2178 - valid_acc: 96.7500\n",
      "val_acc did not improve from 97.1250\n",
      "Epoch: 026 - train_loss: 2.9671 - train_acc: 98.7500 - valid_loss: 3.1359 - valid_acc: 97.0000\n",
      "val_acc did not improve from 97.1250\n",
      "Epoch: 027 - train_loss: 2.8998 - train_acc: 100.0000 - valid_loss: 3.0779 - valid_acc: 97.4375\n",
      "val_acc improved from 97.1250 to 97.4375, saving model to DMC.pt\n",
      "Epoch: 028 - train_loss: 2.8415 - train_acc: 100.0000 - valid_loss: 3.0278 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 029 - train_loss: 2.7935 - train_acc: 100.0000 - valid_loss: 2.9877 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 030 - train_loss: 2.7580 - train_acc: 100.0000 - valid_loss: 2.9551 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 031 - train_loss: 2.7316 - train_acc: 100.0000 - valid_loss: 2.9398 - valid_acc: 96.8125\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 032 - train_loss: 2.7052 - train_acc: 100.0000 - valid_loss: 2.9115 - valid_acc: 97.0000\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 033 - train_loss: 2.6751 - train_acc: 100.0000 - valid_loss: 2.8984 - valid_acc: 96.6250\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 034 - train_loss: 2.6539 - train_acc: 100.0000 - valid_loss: 2.8626 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 035 - train_loss: 2.6326 - train_acc: 100.0000 - valid_loss: 2.8497 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 036 - train_loss: 2.6157 - train_acc: 100.0000 - valid_loss: 2.8299 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.4375\n",
      "Epoch: 037 - train_loss: 2.5995 - train_acc: 100.0000 - valid_loss: 2.8120 - valid_acc: 97.6250\n",
      "val_acc improved from 97.4375 to 97.6250, saving model to DMC.pt\n",
      "Epoch: 038 - train_loss: 2.5833 - train_acc: 100.0000 - valid_loss: 2.7978 - valid_acc: 97.6250\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 039 - train_loss: 2.5718 - train_acc: 100.0000 - valid_loss: 2.7880 - valid_acc: 97.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 040 - train_loss: 2.5579 - train_acc: 100.0000 - valid_loss: 2.7764 - valid_acc: 97.5625\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 041 - train_loss: 2.5486 - train_acc: 100.0000 - valid_loss: 2.7774 - valid_acc: 97.0000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 042 - train_loss: 2.5410 - train_acc: 100.0000 - valid_loss: 2.8216 - valid_acc: 96.0000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 043 - train_loss: 2.5659 - train_acc: 99.3750 - valid_loss: 2.7604 - valid_acc: 97.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 044 - train_loss: 2.5918 - train_acc: 100.0000 - valid_loss: 2.8397 - valid_acc: 96.8125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 045 - train_loss: 2.6434 - train_acc: 98.7500 - valid_loss: 2.7896 - valid_acc: 96.9375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 046 - train_loss: 2.5677 - train_acc: 99.3750 - valid_loss: 2.7266 - valid_acc: 97.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 047 - train_loss: 2.5341 - train_acc: 100.0000 - valid_loss: 2.7285 - valid_acc: 97.0625\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 048 - train_loss: 2.5144 - train_acc: 100.0000 - valid_loss: 2.7061 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 049 - train_loss: 2.5001 - train_acc: 100.0000 - valid_loss: 2.7064 - valid_acc: 97.1250\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 050 - train_loss: 2.4900 - train_acc: 100.0000 - valid_loss: 2.6971 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 051 - train_loss: 2.4809 - train_acc: 100.0000 - valid_loss: 2.6903 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 052 - train_loss: 2.4735 - train_acc: 100.0000 - valid_loss: 2.6746 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 053 - train_loss: 2.4666 - train_acc: 100.0000 - valid_loss: 2.6688 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 054 - train_loss: 2.4618 - train_acc: 100.0000 - valid_loss: 2.6608 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 055 - train_loss: 2.4576 - train_acc: 100.0000 - valid_loss: 2.6578 - valid_acc: 97.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 056 - train_loss: 2.4556 - train_acc: 100.0000 - valid_loss: 2.6601 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 057 - train_loss: 2.4530 - train_acc: 100.0000 - valid_loss: 2.6657 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 058 - train_loss: 2.4516 - train_acc: 100.0000 - valid_loss: 2.6562 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 059 - train_loss: 2.4490 - train_acc: 100.0000 - valid_loss: 2.6660 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 060 - train_loss: 2.4473 - train_acc: 100.0000 - valid_loss: 2.6552 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 061 - train_loss: 2.4431 - train_acc: 100.0000 - valid_loss: 2.6548 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 062 - train_loss: 2.4410 - train_acc: 100.0000 - valid_loss: 2.6499 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 063 - train_loss: 2.4390 - train_acc: 100.0000 - valid_loss: 2.6497 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 064 - train_loss: 2.4356 - train_acc: 100.0000 - valid_loss: 2.6442 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 065 - train_loss: 2.4331 - train_acc: 100.0000 - valid_loss: 2.6500 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 066 - train_loss: 2.4305 - train_acc: 100.0000 - valid_loss: 2.6316 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 067 - train_loss: 2.4297 - train_acc: 100.0000 - valid_loss: 2.6367 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 068 - train_loss: 2.4286 - train_acc: 100.0000 - valid_loss: 2.6269 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 069 - train_loss: 2.4267 - train_acc: 100.0000 - valid_loss: 2.6342 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 070 - train_loss: 2.4268 - train_acc: 100.0000 - valid_loss: 2.6223 - valid_acc: 97.4375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 071 - train_loss: 2.4395 - train_acc: 100.0000 - valid_loss: 2.8466 - valid_acc: 96.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 072 - train_loss: 2.6601 - train_acc: 99.3750 - valid_loss: 2.8939 - valid_acc: 92.5625\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 073 - train_loss: 2.6883 - train_acc: 97.5000 - valid_loss: 2.7892 - valid_acc: 96.9375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 074 - train_loss: 2.5382 - train_acc: 100.0000 - valid_loss: 2.7770 - valid_acc: 96.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 075 - train_loss: 2.5008 - train_acc: 100.0000 - valid_loss: 2.7255 - valid_acc: 97.0000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 076 - train_loss: 2.4863 - train_acc: 100.0000 - valid_loss: 2.7284 - valid_acc: 96.6875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 077 - train_loss: 2.4797 - train_acc: 100.0000 - valid_loss: 3.0915 - valid_acc: 93.1250\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 078 - train_loss: 2.6170 - train_acc: 98.1250 - valid_loss: 2.8207 - valid_acc: 96.0000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 079 - train_loss: 2.5637 - train_acc: 98.1250 - valid_loss: 2.6886 - valid_acc: 96.9375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 080 - train_loss: 2.5144 - train_acc: 98.7500 - valid_loss: 2.7155 - valid_acc: 96.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 081 - train_loss: 2.5317 - train_acc: 98.7500 - valid_loss: 2.7091 - valid_acc: 95.0625\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 082 - train_loss: 2.4842 - train_acc: 99.3750 - valid_loss: 2.6935 - valid_acc: 96.8125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 083 - train_loss: 2.4754 - train_acc: 100.0000 - valid_loss: 2.6634 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 084 - train_loss: 2.4538 - train_acc: 100.0000 - valid_loss: 2.6439 - valid_acc: 97.1250\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 085 - train_loss: 2.4412 - train_acc: 100.0000 - valid_loss: 2.6326 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 086 - train_loss: 2.4338 - train_acc: 100.0000 - valid_loss: 2.6165 - valid_acc: 97.4375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 087 - train_loss: 2.4276 - train_acc: 100.0000 - valid_loss: 2.6163 - valid_acc: 97.5000\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 088 - train_loss: 2.4244 - train_acc: 100.0000 - valid_loss: 2.6228 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 089 - train_loss: 2.4220 - train_acc: 100.0000 - valid_loss: 2.6183 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 090 - train_loss: 2.4204 - train_acc: 100.0000 - valid_loss: 2.6186 - valid_acc: 97.3125\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 091 - train_loss: 2.4185 - train_acc: 100.0000 - valid_loss: 2.6242 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 092 - train_loss: 2.4179 - train_acc: 100.0000 - valid_loss: 2.6214 - valid_acc: 97.1250\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 093 - train_loss: 2.4167 - train_acc: 100.0000 - valid_loss: 2.6135 - valid_acc: 97.3750\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 094 - train_loss: 2.4163 - train_acc: 100.0000 - valid_loss: 2.6119 - valid_acc: 97.4375\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 095 - train_loss: 2.4158 - train_acc: 100.0000 - valid_loss: 2.6280 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 096 - train_loss: 2.4177 - train_acc: 100.0000 - valid_loss: 2.6236 - valid_acc: 97.0625\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 097 - train_loss: 2.4141 - train_acc: 100.0000 - valid_loss: 2.6106 - valid_acc: 97.1875\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 098 - train_loss: 2.4127 - train_acc: 100.0000 - valid_loss: 2.6116 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 099 - train_loss: 2.4117 - train_acc: 100.0000 - valid_loss: 2.6181 - valid_acc: 97.2500\n",
      "val_acc did not improve from 97.6250\n",
      "Epoch: 100 - train_loss: 2.4114 - train_acc: 100.0000 - valid_loss: 2.6222 - valid_acc: 97.1250\n",
      "val_acc did not improve from 97.6250\n",
      "Running Time: 287.70\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "print('start training')\n",
    "acc_list = [0.00]\n",
    "path = './model/DMC.pt'\n",
    "tic = time.time()\n",
    "for epoch in range(1, args.epoches+1):\n",
    "    # 计算的是移动平均准确率\n",
    "    train_acc, train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    valid_acc, valid_loss = valid(model, valid_loader, criterion, epoch)\n",
    "    print(\"Epoch: {:03d} - train_loss: {:.4f} - train_acc: {:.4f} - valid_loss: {:.4f} - valid_acc: {:.4f}\".\\\n",
    "          format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    scheduler.step()\n",
    "\n",
    "    acc_list.append(valid_acc)\n",
    "    if acc_list[-1] > acc_list[-2]:\n",
    "        print(\"val_acc improved from {:.4f} to {:.4f}, saving model to DMC.pt\".format(acc_list[-2], acc_list[-1]))\n",
    "        torch.save(model.state_dict(), path)\n",
    "    else:\n",
    "        print(\"val_acc did not improve from {:.4f}\".format(acc_list[-2]))\n",
    "        acc_list[-1] = acc_list[-2]\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Running Time: {:.2f}\".format(toc-tic))\n",
    "print(\"**************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "tar_test, pre_test = test(model, test_loader)\n",
    "OA_test, AA_mean_test, Kappa_test, AA_test = output_metric(tar_test, pre_test)\n",
    "AA_test = np.around(AA_test*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************\n",
      "Final result:\n",
      "OA: 93.78, AA: 97.34, Kappa: 0.9309\n",
      "*******************************************************************\n",
      "Recal: [ 93.   100.   100.   100.    98.05 100.   100.    78.4  100.    98.04\n",
      " 100.   100.   100.   100.    91.02  98.94]\n",
      "*******************************************************************\n",
      "Parameter:\n",
      "dataset: Salinas\n",
      "seed: 42\n",
      "batch_size: 32\n",
      "patch_size: 15\n",
      "epoches: 100\n",
      "learning_rate: 0.005\n",
      "gamma: 0.99\n",
      "weight_decay: 0.001\n",
      "train_number: 10\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"*******************************************************************\")\n",
    "print(\"Final result:\")\n",
    "print(\"OA: {:.2f}, AA: {:.2f}, Kappa: {:.4f}\".format(OA_test * 100., AA_mean_test*100., Kappa_test))\n",
    "print(\"*******************************************************************\")\n",
    "print(\"Recal: {}\".format(AA_test))\n",
    "print(\"*******************************************************************\")\n",
    "print(\"Parameter:\")\n",
    "print_args(vars(args))\n",
    "print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = str(list(AA_test))\n",
    "file_name = \"./CR/dmcn_1.txt\"\n",
    "\n",
    "with open(file_name, 'w') as x_file:\n",
    "    x_file.write(\"[{:.2f}, {:.2f}, {:.4f}]\".format(OA_test * 100., AA_mean_test*100., Kappa_test))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoramic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(model, loader):\n",
    "    model.eval()\n",
    "    pre = np.array([]).astype('int')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_data, batch_target) in enumerate(loader):\n",
    "            batch_data = batch_data.cuda()\n",
    "            batch_pred = model(batch_data)  # (B, 9)\n",
    "            _, pred = batch_pred.topk(1, axis=1)  # (B, 1)\n",
    "            pp = pred.squeeze()\n",
    "            pre = np.append(pre, pp.data.cpu().numpy())\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "x_forward shape = (54129, 15, 15, 30), type = float64\n",
      "x_backward  shape = (56975, 15, 15, 30), type = float64\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "pos_forward, pos_backward = choose_all_pixels(label)\n",
    "\n",
    "x_forward, x_backward, y_forward, y_backward = all_data(mirror_data, band, pos_forward, pos_backward, patch_size=args.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54129, 30, 15, 15])\n",
      "torch.Size([56975, 30, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_forward = torch.from_numpy(x_forward.transpose(0, 3, 1, 2)).type(torch.FloatTensor)\n",
    "print(x_forward.shape)\n",
    "y_forward = torch.from_numpy(y_forward).type(torch.LongTensor)  # (13,)\n",
    "forward_label = Data.TensorDataset(x_forward, y_forward)\n",
    "\n",
    "x_backward = torch.from_numpy(x_backward.transpose(0, 3, 1, 2)).type(torch.FloatTensor)\n",
    "print(x_backward.shape)\n",
    "y_backward = torch.from_numpy(y_backward).type(torch.LongTensor)  # (13,)\n",
    "backward_label = Data.TensorDataset(x_backward, y_backward)\n",
    "\n",
    "forward_loader = Data.DataLoader(forward_label, batch_size=128, shuffle=False)\n",
    "backward_loader = Data.DataLoader(backward_label, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output classification maps\n",
    "pre_forward = output_label(model, forward_loader)\n",
    "pre_backward = output_label(model, backward_loader)\n",
    "\n",
    "prediction_matrix = np.zeros((height, width), dtype=float)\n",
    "for i in range(pos_forward.shape[0]):\n",
    "    prediction_matrix[pos_forward[i, 0], pos_forward[i, 1]] = pre_forward[i] + 1\n",
    "\n",
    "for j in range(pos_backward.shape[0]):\n",
    "    prediction_matrix[pos_backward[j, 0], pos_backward[j, 1]] = pre_backward[j] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 104.16x245.76 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_map = list_to_colormap(prediction_matrix.ravel())\n",
    "predict_map = np.reshape(predict_map, (height, width, 3))\n",
    "\n",
    "classification_map(predict_map, './salinas_dmcn.pdf')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[FINAL] IndianPinesSeed3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
